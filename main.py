# -*- coding: utf-8 -*-
"""devolution.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1PGxSs7O4m2W47ukUW0hrTZsgwuA3dNeb
"""

import pandas as pd
import pyodbc
import torch
import torch.utils.data as data_utils
from torch.utils.tensorboard import SummaryWriter
import numpy as np
from devolution_network import DevolutionNetwork
from utils import save_model, load_model

conn = pyodbc.connect(DRIVER='{ODBC Driver 17 for SQL Server}',
                      SERVER='devohackaton.database.windows.net',
                      DATABASE='privilege',
                      UID='aadmin',
                      PWD='9rTuUjFcssBUGws2')

cursor = conn.cursor()

bdd_data = pd.read_sql_query('SELECT * FROM [privilege].[dbo].[userdetailtrains]', conn)
credit_scores = pd.read_sql_query(
    'SELECT [UserId],[Amount],[Probability] FROM [privilege].[dbo].[RepaymentProbabilities]', conn)


# merge user data and creditscore
data = []
for index, row in credit_scores.iterrows():
    client_detail = bdd_data[bdd_data.UserId == row["UserId"]]
    new_row = [client_detail.iloc[0][col] for col in bdd_data.head()]
    new_row += [row["Amount"], row["Probability"]]
    data.append(new_row)

data = pd.DataFrame(data, columns=list(bdd_data.head()) + ["Amount", "Probability"])
del data["CreatedByUserId"]
del data["UserId"]
del data["CreditScore"]
del data["CreationDate"]
del data["Id"]

# noisy data
del data["Income"]
del data["JobClass"]
del data["EmploymentType"]
del data["SocialStability"]
del data["SocialExposure"]
del data["SocialQuality"]

pd.set_option("display.max_rows", None, "display.max_columns", None)
print(data)
data = data.iloc[:3]
exit()
# exit()

# preprocessing the data
data = data.replace("M", 0)
data = data.replace("F", 1)
data = data.replace(True, 1)
data = data.replace(False, 0)



dff = data.drop('Probability', axis=1)
mean = dff.mean().values
std = dff.std().values

# compute mean and variance of the data


ratio = 0.75
n_train = int(ratio * data.shape[0])

input_size = data.shape[1] - 1

train_df = data.iloc[:n_train]
test_df = data.iloc[n_train:]


def create_dataset(dataframe):
    y = torch.tensor(dataframe['Probability'].values.astype(np.float32))
    x = torch.tensor(dataframe.drop('Probability', axis=1).values.astype(np.float32))
    dataset = data_utils.TensorDataset(x, y)
    return dataset


trainset = create_dataset(train_df)
testset = create_dataset(test_df)

mini_batch_size = n_train

trainloader = torch.utils.data.DataLoader(
    trainset,
    batch_size=mini_batch_size,
    shuffle=True, num_workers=1)
testloader = torch.utils.data.DataLoader(
    testset,
    batch_size=mini_batch_size,
    shuffle=False, num_workers=1)

hidden_layer_size = [32]
net = DevolutionNetwork(input_size, hidden_layer_size, 1, mean, std)
print(net)

criterion = torch.nn.L1Loss()
optimizer = torch.optim.Adam(net.parameters(), lr=0.001)  # 0.2)
stopping_loss = 0.001

# load data from loader
writer = SummaryWriter()

for epoch in range(10000):  # loop over the dataset multiple times

    for i, (inputs, labels) in enumerate(trainloader):  # loop over minibatches
        optimizer.zero_grad()
        outputs = net(inputs)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()
        minibatch_loss = loss.item()

        save_model(net, "model")
        writer.add_scalar("minibatch_loss", minibatch_loss, epoch)

    if epoch % 100 == 0:
        for label, dataloader in [("train", trainloader), ("test", testloader)]:
            print("============ {} ===========".format(label))
            loss = 0
            for i, (inputs, labels) in enumerate(dataloader):  # loop over minibatches
                optimizer.zero_grad()
                outputs = net(inputs)
                for i_sample, (p, l) in enumerate(zip(outputs, labels)):
                    print("[epoch={}][{}] y={} y_pred={} loss={}"
                          .format(epoch, i_sample, l, p, criterion(l, p)))
                loss += criterion(outputs, labels)
            loss /= (i + 1)
            writer.add_scalar("Loss/{}".format(label), loss, epoch)
        save_model(net, "model")

        # if running_loss < stopping_loss:
        #     break
writer.flush()

print('Finished Training')
save_model(net, "model")
net = load_model("model")

writer.close()

# import matplotlib.pyplot as plt
# import numpy as np
#
# plt.scatter(x.data.numpy(), y.data.numpy(), color = "orange")
# plt.plot(x.data.numpy(),net(x).data.numpy())
#
# plt.show()

# from google.colab import files
# files.download('model.pth')
